ğŸ–ï¸ Hand Gesture Recognition for Drone Control
This project uses MediaPipe, OpenCV, and TensorFlow to recognize hand gestures in real-time from a webcam feed. Recognized gestures can be mapped to control commands for a drone (e.g., DJI Tello).

ğŸ“‚ Folder Structure

hand_gesture_control/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ gestures.csv              # Collected landmark data
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ gesture_model_mediapipe.h5  # Trained ML model
â”‚   â””â”€â”€ label_encoder.pkl           # Encoder for gesture labels
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ collect_landmarks.py     # Collect labeled hand landmark data
â”‚   â”œâ”€â”€ train_model.py           # Train gesture recognition model
â”‚   â””â”€â”€ realtime_predict.py      # Real-time gesture prediction from webcam
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # Project documentation

ğŸš€ How It Works
MediaPipe extracts 21 hand landmarks per frame.

These 3D coordinates are used to train a classifier.

The model is trained on gesture-labeled landmark data.

In real-time, the model predicts the gesture and displays it on screen.

(Optional) Detected gestures can trigger drone commands.

ğŸ› ï¸ Setup Instructions
1. Clone the Repository

git clone https://github.com/your-username/hand_gesture_control.git
cd hand_gesture_control
2. Create a Virtual Environment

python -m venv venv
venv\Scripts\activate   # On Windows
3. Install Dependencies

pip install -r requirements.txt
ğŸ“¸ Step 1: Collect Gesture Data
Run this to collect labeled landmark data for one gesture:

python scripts/collect_landmarks.py
Adjust the GESTURE_LABEL variable inside the script before running.

Press s to save a sample, and q to quit.

Repeat for each gesture (e.g., fist, open_palm, left, right, etc.).

ğŸ§  Step 2: Train the Model
After collecting all gesture samples:

python scripts/train_model.py
This will generate:

models/gesture_model_mediapipe.h5

models/label_encoder.pkl

ğŸ•µï¸ Step 3: Real-Time Gesture Detection
Use your webcam to test the model in real-time:

python scripts/realtime_predict.py
Predicted gestures will appear on the video frame.

Flickering is reduced using a smoothing buffer.

ğŸ›¸ (Optional) Drone Control
If you're using a DJI Tello drone, you can extend realtime_predict.py to send commands using djitellopy:

pip install djitellopy
Example usage inside the frame loop:

if most_common == "left":
    tello.move_left(30)
elif most_common == "fist":
    tello.land()
ğŸ“¦ Dependencies
Listed in requirements.txt